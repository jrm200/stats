---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The defined parameters are passed as arguments to the log posterior function below. $\log(\sigma_b)$ follows an exponential prior distribution with rate $5$ and the remaining parameters have uniform prior distributions. The function \texttt{lfyb} that was previously computed has been made positive and added to the prior for $\log(\sigma_b)$.

```{r}
log.posterior <- function (b, theta, times = y, X.mat=X, Z.mat=Z) {
  #log likelihood
  prior <- dexp(x=exp(theta[4]), rate=5, log=TRUE) #define log(sigb) to follow a exponentional prior distn with rate 5
  prior[prior==-Inf] <- -10
  l.lik <- -lfyb(b, times, theta, X.mat, Z.mat) + prior #add the absolute values of the log likelihood together
  l.lik
}
```

The metropolis Hastings algorithm is implemented with the code below. The proposal distributions used

```{r}
n.rep <- 100000
sigma.prop <- c(0.15, 0.055, 0.055, 0.1) #tuning possible; (0.15, 0.055, 0.055, 0.1) 
#looks to be very good in terms of acceptance rate

MH <- function (theta, sigma.prop, n.rep) {
  theta.vals <- matrix(0, n.rep, 4) #matrix to save generated values
  theta.vals[1,] <- theta
  b <- rep(0, 50)
  b.vals <- matrix(0, n.rep, 50)
  b.vals[1,] <- b
  lp0 <- log.posterior(b, theta.vals[1,])
  accept.th <- 0
  accept.b <- 0

  for (i in 2:n.rep) {
    #update theta
    theta <- theta+rnorm(4, 0, sigma.prop)
    lp1 = log.posterior(b, theta)#evaluate log posterior
    if (runif(1) < exp(lp1 - lp0)){#accept step with probability alpha
      accept.th <- accept.th+1
      lp0 <- lp1
    }else{
      theta <- theta.vals[i-1,]
    }
    #update random effects
    b <- b + rnorm(50)*0.045 #after tuning, 0.045 produces an acceptance rate around 0.23
    lp1 <- log.posterior(b, theta)
    if (runif(1) < exp(lp1 - lp0)){
      accept.b <- accept.b+1
      lp0 <- lp1
    }else{
      b <- b.vals[i-1,]
    }
    theta.vals[i,] <- theta
    b.vals[i,] <- b
  }
  accept.rate <- c(accept.th/n.rep, accept.b/n.rep)
  list(theta=theta.vals, accept.rate=accept.rate)
}
```

```{r}
#initialize theta to an arbitrary value; run Metropolis Hastings
theta0 <- c(1,0,0.1,0.2)
mh <- MH(theta0, sigma.prop, n.rep)
theta <- mh$theta
accept.rate <- mh$accept.rate
```
Plots below.
```{r, echo=FALSE, include=FALSE, fig.height=2.5}
#we plot the autocorrelation of the variables, relation between elements of theta and the evolution of each element of theta
par(mfrow=c(2,2),mar=c(4,4,1,1))
acf(theta[,4], lag.max=2000) #looks like the 'stickiest' parameter is theta[4], acf = 0 after about 2000 iterations
acf(theta[,3], lag.max=2000)
acf(theta[,2], lag.max=2000)
acf(theta[,1], lag.max=2000)

lower <- n.rep/5 + 1
theta <- theta[lower:n.rep,]

par(mfrow=c(3,2),mar=c(4,4,1,1))
plot(theta[,1],theta[,2],xlab="B0",ylab="B1",pch=20,cex=1)
plot(theta[,1],theta[,3],xlab="B0",ylab="log(sig)",pch=20,cex=1)
plot(theta[,1],theta[,4],xlab="B0",ylab="log(sig.b)",pch=20,cex=1)
plot(theta[,2],theta[,3],xlab="B1",ylab="log(sig)",pch=20,cex=1)
plot(theta[,2],theta[,4],xlab="B1",ylab="log(sig.b)",pch=20,cex=1)
plot(theta[,3],theta[,4],xlab="log(sig)",ylab="log(sig.b)",pch=20,cex=1)

par(mfrow=c(2,2),mar=c(4,4,1,1))
for (i in 1:4){
  plot(theta[,i])
}
```
Need to talk about the plots. 

## Fatigue of materials

### Absract

A number of fatigue tests have been carried out on materials which are subject to cyclic loading. 

This study has been set up as a means of predicting the fatigue, as to aid the use of fatigue materials within the practice of mechanical and structural engineering. 

### Introduction

The fatigue characteristics are established through $26$ fatigue tests which are performed on the small flat plates of the loading materials called coupons. The following elements of the study were recorded.

*   $\texttt{Cycles}$ the number of cycles, $\N_i$ until failure in coupon $i$.    
*   $\texttt{Stress levels}$ the controlled levels of stress, $s_i$ have been recorded as force per unit area (in Mega Pascals, MPa).    

The stress levels $s_i$ are controlled throughout the series of tests, whilst the number of cycles to failure $N_i$ exhibits a random behaviour due to inherent microstructural inhomogeneity in the material properties as well as due to uncontrolled differences in test conditions. If a test is terminated before the coupon fails, then the coupon is market as a 'runout'. In terms of likelihoods, the runouts can be interpreted as censored observations.

### Methods

The number of cycles has been modelled using the following probability model.

$$ N_i = \alpha (s_i - \gamma)^{\delta} \epsilon_i, \ \ \ \ \ \ \ \textrm{where} \ \ s_i > \gamma $$

and $\epsilon_i$ is a random error such that,

$$ \epsilon_i \sim \textrm{Weibull}(\textrm{shape}= 1/ \sigma, \ \textrm{scale}=1) $$
The constants $\alpha > 0$, $\delta \in \mathbb{R}$, $\delta > 0 $ and $\sigma > 0 $ are unknown parameters. Empirical results suggest coupons tested below the stress level $\gamma$ will never fail. The unknown parameter $\gamma$ is therefore called the \textbf{fatigue limit}.

Primarily, $\gamma$ was taken to be $50$ so that the runouts will be ommitted. The maximum likelihood estimate of $\boldsymbol{\theta}= (\log(\alpha), \delta, \log(\sigma))^T$, was found using \texttt{optim}. The $R$ code constructed to compute this is displayed.

```{r}
fatigue<- read.table("http://people.bath.ac.uk/kai21/ASI/fatigue.txt")
#need to show the Ni is a weibull dist with shape epsilon and scale alpha(s-gama)^delta,
#then can compute likelihood as in Q1.
#Question 2.1
gama <- 50 #arbitrary gamma
theta <- c(1,1,1) #arbitrary initial theta
s <- fatigue$s #stress
N <- fatigue$N

nll.weibull <- function (theta, gama, s, N) {
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1])*(s - gama)^theta[2] #scale
  ro <- fatigue$ro
  #compute the negative log likelihood
  nll0 <- -sum(dweibull(N, shape=k, scale=lambda, log=TRUE)^(1-ro))
  nll1 <- sum(((N/lambda)^k)^ro)
  nll <- nll0 + nll1
  nll
}
maxlik <- optim(theta, nll.weibull, gama=gama, s=s, N=N, method="BFGS", hessian=TRUE)
```

The returned objects can be seen below.

```{r, echo=FALSE, message=FALSE}
maxlik
```

The optimised vector of parameters $\boldsymbol{\hat{\theta}}= (13.5727280, -1.0460720, -0.5109903)$. The asymptotic $95\%$ confidence intervals of the parameters are displayed in columns $1$, $2$ and $3$ respectively below.

```{r,echo=FALSE, message=FALSE}
se <- sqrt(diag(solve(maxlik$hessian)))
CI <- matrix(c(CI1,CI2,CI3), ncol=3, nrow=2)
CI
```

$\gamma$ was then varied in order to identify the impact it has on the sensitivity of the results. The code that has been displayed below and following this, plots were made in orer to observe the trend.

```{r}
#How variations in gama affects the theta
f.vals <- rep(0,80)
f.par <- matrix(0,nrow=3, ncol=80)
for (i in 1:80) {
  gama <- i
  f.vals[i] <- nll.weibull(theta, gama, s, N)
  maxlik <- optim(theta, nll.weibull, gama=gama, s=s, N=N, hessian=TRUE)
  f.par[,i] <- maxlik$par
}

```

```{r, echo=FALSE, message=FALSE, fig.height=3} 
par(mfrow=c(1,3))
gama <- seq(from=1,to=80)
plot(gama, f.par[1,], ylab="log(alpha)", xlab="gamma")
plot(gama, f.par[2,], ylab="delta", xlab="gamma")
plot(gama, f.par[3,], ylab="log(sigma)", xlab="gamma")
```

There seems to be a downward linear trend amongst $\log(\alpha)$ and $\gamma$. As the values for $\gamma$  increase from $0$ to $80$, $\log(\alpha)$ decreases from around $40$ for $\gamma=0$. There is an apparent upward linear trend amongst $\delta$ for increasing $\gamma$. It varies from around $-6$ to around $-1$. As $\gamma$ increases, $\delta$ appears to relatively steadily decline from around $-0.8$ to $-0.9$ and then randomly seems to increase exponentially. It appears that changes in $\gamma$ impact $\delta$ quite substantially. This parameter is sensitive to change in $\gamma$.

$\gamma$ has been estimated amongst the vector of unknowns $\boldsymbol{\theta}_* = (\log(\alpha), \delta, \log(\sigma), \gamma)$. This has been executed using the $R$ code below.

```{r}
#Q7 we can estimate gama by including it within theta and passing it to optim
#redefine nll.weibull with theta[4] = gama
gama <- 50 #reset gama
theta.opt <- c(theta.opt, gama)

nll.weibull2 <- function (theta, s, N) {
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1])*(s - theta[4])^theta[2] #scale
  ro <- fatigue$ro
  #compute the negative log likelihood
  nll0 <- -sum(dweibull(N, shape=k, scale=lambda, log=TRUE)^(1-ro))
  nll1 <- sum(((N/lambda)^k)^ro)
  nll <- nll0 + nll1
  nll
}

maxlik2 <- optim(theta.opt, nll.weibull2, s=s, N=N, hessian=TRUE)
theta.opt2 <- maxlik2$par
gama.opt <- theta.opt2[4] #gama looks like it should be around 66
```
It has been done in such a way that $\gamma$ has been passed to the vector of parameters which are modelled using a Weibull distribution. The new vector of parameters $\boldsymbol{\theta}_*$ which ultimately will be passed to \texttt{optim} to compute the MLE. The estimates of the new vector of parameters is displayed below.

```{r, echo=FALSE, message=FALSE}
theta.opt2 <- maxlik2$par
theta.opt2
```
Hence, with $\gamma$ as an added parameter it has been estimated to be around 66.

We proceed to focus on the lower quantiles of the fatigue as a function of stress as this is an area of great interest within engineering. The lower $10\%$ quantile of $N$ is considered using the following probability model.

$$N_{0.1} = \alpha (s_i - \gamma)^{\delta}z^\sigma_{0.1}   $$
Where $z_{0.1}$ is the lower quantile of a Weibull distribution with unit shape and scale. This will be such that it is am exponential distribution with unit rate. The following $R$ code displays the construction of the $10\%$ quantile. 
```{r}
#function to compute quantile of N
N.quantile <- function (theta, s, quantile) {
  ZQ <- qweibull(quantile, shape=1, scale=1)
  NQ <- exp(theta[1])*(s-theta[4])^theta[2]
  Q <- NQ*ZQ^exp(theta[3])
  Q
}

plot(s, log(N))
S <- seq(from=min(s), to=max(s), by=0.1)
N.lower.quantile <- N.quantile(theta=theta.opt2, s=s, quantile=0.1)
lines(s, log(N.lower.quantile), col="red")
N.middle.quantile <- N.quantile(theta=theta.opt2, s=s, quantile=0.5)
lines(s, log(N.middle.quantile), col="green")
```

The plot below displays the log transformed data points along with interpolated $10\%$ and $\50%$ quantiles of $N$.

```{r, echo=FALSE, message=FALSE, fig.height=3}
par(mfrow=c(1,1))
plot(s, log(N))
S <- seq(from=min(s), to=max(s), by=0.1)
N.lower.quantile <- N.quantile(theta=theta.opt2, s=s, quantile=0.1)
lines(s, log(N.lower.quantile), col="red")
N.middle.quantile <- N.quantile(theta=theta.opt2, s=s, quantile=0.5)
lines(s, log(N.middle.quantile), col="green")

```

It can be seen that the estimated MLE parameters calculated fits the observed data quite well, however there are some obvious outliers that fall below the $10\%$ quantile of the curve.


