---
title: "MA40198: Applied Statistical Inference"
author: "Candidates: 07604, 07614, 07615"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Carcinogenesis Study on Rats

### Abstract

Following a drug trial that was carried out on 150 rats, this study seeks to achieve an answer as to whether the method of cancer treatment is effective or not. 

Under the conditions of this study, there was very little to no evidence suggesting that the treatment used was a preventer of cancer within the observed population of rats.

### Introduction

The recorded data includes the variables: __Litter__- comprised of three rats pertaining to fifty different litters, __Treatment Indicator__- when $rx=1$ indicates the treatment was carried out on the rat and where $rx=0$ indicates the rat received the control/placebo only, __Time__- the follow up time, in weeks, to the appearance of a tumor, __Status__- status$=1$ describes that the presence of a tumor and status$=0$ implies the rat died before the presence of a tumor.

The drug trial was carried out on solely female rats in a controlled environment and saw one in three of the rats per litter being treated for cancer whilst the other two received a placebo "treatment" only. Following the treatment, the rats were detained and observed. If a tumor appeared on a rat or if the rat died, the occurance was recorded at the specific time in weeks, indicating which of the two outcomes occurred. This drug trial was carried out with the motivation to discover whether or not the method of cancer treatment used was effective or not.

Using Maximum Likelihood Estimation, this study is comprised of two main approaches of statistical analyses: frequentist estimation procedures and Bayesian estimation procedures. Using both the Bayesian and Frequentist approach, probability models for the follow up time and the survival function have been distributed using the Weibull function. A Frequentist approach was also taken whilst constructing a probability model for the data, using a log-logistic distribution. 

### Methods
###detailed explanation of methods (Second section)
* Include Weibull, loglik, weibull w/ random effects-- marginal neg loglik on laplace &joint log density

Upon the first approach to the MLE problem, the dependent variable time described by $T_i$, has been taken to follow a Weibull distribution as we cannot assume that the data does not complies with the assumptions of normality. The distribution is as below.
$$ T_i \sim \mbox{Weibull}(\mbox{shape} = \frac{1}{\sigma}, \mbox{scale}=\exp(\eta_i)), \ \ \ \ \ \ \ \ \eta_i=\beta_0 + \beta_1 x_i $$
where
$$x_i=\left\{
\begin{array}{cl}
1 & \mbox{rat $i$ received treatment} \\
0 & \mbox{rat $i$ received control}\\
\end{array}
\right.$$ 
It can be seen from the above that the shape parameter in the model takes the form of $\frac{1}{\sigma}$ whilst the scale parameter is proportional to $\exp(\beta_0 + \beta_1 x_i)$. Censored observations, where the Treatment Indicator, $rx=0$, implied that the rat received only the control, are described by the survival function of a Weibull distribution given as follows.
$$ S(t|\mbox{a,b})= \exp\bigg(-\bigg( \frac{t}{b}\bigg)^a\bigg), \ \ \ \ \ \ \ \ t> 0 $$ 
The maximum likelihood estimate calculates an estimation of the population parameters such that the likelihood of obtaining the observed data is maximised. The vector of parameters that are to be estimated can be expressed as $\boldsymbol{\theta}^T=( \beta_0, \beta_1, \log(\sigma))$.

Given all observations $t_i$ are independent of one another, which we can assume to be true, the likelihood of the whole sample $T_i$ is the product of the individual likelihoods over all observations $t_i$. Where $L$ is the likelihood, it can be defined as follows.

$$ L=L_1 \times L_2 \times ... \times L_N = \prod^N_{i=1}{L_i} = P(y_1| \boldsymbol{\hat{\theta}}) \times P(y_2| \boldsymbol{\hat{\theta}}) \times ... \times P(y_N| \boldsymbol{\hat{\theta}}) = \prod^N_{i=1} P(y_i|\boldsymbol{\hat{\theta}} ) $$
It is true that the likelihood will always be positive and since the $\log$ function is a non decreasing function, if we transform the likelihood by taking the $\log$ of it, it's maxima will remain in the same position. When working with $\log$ it is more versatile as the sum of the log of the likelihood can be taken.

A function to describe the likelihood of the parameters $\boldsymbol{\theta}$ is defined in $R$ as below.


```{#numCode .R .numberLines}
nll.weibull <- function (theta) {
  #create subset of data since for the purposes of calculating the optimal theta since 
  #when status = 0, the survival function does not depend on theta. Need to optimise this 
  #function using optim.
  t1 <- rats[which(rats$status==1),,]$time
  rx1 <- rats[which(rats$status==1),,]$rx
  t0 <- rats[which(rats$status==0),,]$time
  rx0 <- rats[which(rats$status==0),,]$rx
  #compute the negative log likelihood
  k <- 1/exp(theta[3]) #shape 
  lambda <- exp(theta[1] + theta[2]*rx0) #scale with censoring
  loglik1 <- -sum(dweibull(t1, shape=1/exp(theta[3]), scale=exp(theta[1] + theta[2]*rx1), log=TRUE))
  loglik2 <- sum((t0/lambda)^k)
  loglik <- loglik1 + loglik2
  loglik
}
```


Initially, the function describing the negative log likelihood that will be maximised has been defined as \texttt{nll.weibull}. This function includes subsetted data, so that the optimal $\boldsymbol{\theta}$ can be calculated. The use of subsetting is due to the fact that the survival function $S(t| \mbox{a,b})$ does not depend on $\boldsymbol{\theta}$ when the rats died prior to appearance of a tumor. These subsets are defined in lines 5-8 in the $R$ code above. Variable $\boldsymbol{t_0}$ defines a vector of the treatment indicator with respect to only the rats that did not die before the appearance of a tumor, $\boldsymbol{rx_1}$ defines a vector the treatment indicator with respect to only the rats that did not die before the appearance of a tumor, $\boldsymbol{t_1}$ defines a vector of the treatment indicator with respect to only the rats that died before the appearance of a tumor and lastly, $\boldsymbol{rx_0}$ defines a vector of the treatment indicator with respect to only the rats that died.

In order to compute the log likelihood for the censored observations, the shape and scale parameters of the survival function have been defined as below. Given that the third element in $\boldsymbol{\theta}$ is proportional to $\log(\sigma)$, the exponent of $\sigma$ has been taken.
$$ \mbox{Shape}=\frac{1}{\exp(\sigma)}, \ \ \ \ \ \mbox{Scale} = \exp(\beta_0 + \beta_1rx_0) $$
The scale is altered by $rx_0$, since we are considering the treatment indicator based on only censored observations. The shape can be referred to as $k$ whilst the scale can be referred to as $\lambda$. 

Since the method of finding the MLE has utilised the $\mbox{optim}$ function in $R$, the negative log likelihoods are computed, as $\mbox{optim}$ optimises the given function and seeks the minimum value. The likelihood for censored observations has been defined below and can be seen in the $R$ code line 13.

$$ (L) _{rx=0}= - \sum -\log\bigg(\exp\bigg(-\bigg(\frac{t_0}{\lambda}\bigg)^k\bigg) \\
=\sum \bigg(\frac{t_0}{\lambda}\bigg)^k, \ \ \ \ \ \ \ \ t_0 > 0 $$ 

In order to compute the likelihood of the observations in which the appearance of a tumor occured, the shape and scale parameters were passed to the \texttt{dweibull} function built in to $R$. The shape and scale parameters for this probability model are defined below.

$$ \mbox{shape}= \frac{1}{\exp(\sigma)}, \ \ \ \ \ \ \ \ \mbox{scale}=\exp(\beta_0 + \beta_{1} rx_{1}) $$ 
The \texttt{dweibull} function has also been passed $t_1$ as the dependent variable as this distribution is only concerned with the observations that had a tumor appearance. The scale parameter $\beta_0$ is also only altered by the treatment indicator when the rats did not die prior to the appearance of a tumor. The negative sum is taken with the same reasoning as above. This likelihood is defined in the $R$ code in line 12.

The sum of the two different likelihoods are then combined in line 14 to create a combined likelihood for both censored and uncensored observations.

```{#numCode .R .numberLines}
#initialise the variables
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

for (i in 0:1000) {
  theta[1] <- 4.5 + i*0.0025
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.weibull(theta)
}

plot(beta0, beta0var, type="l", xlab=expression(beta[0]), ylab="Variance") #estimate optimal beta0 to be ~5.75

theta <- c(0,0,0)
for (i in 0:1000) {
  theta[2] <- 4 + i*0.0025
  beta1[i+1] <- theta[2]
  beta1var[i+1] <- nll.weibull(theta)
}

plot(beta1, beta1var, type="l", xlab=expression(beta[1]), ylab="Variance") #estimate optimal beta1 to be ~5.25


theta <- c(0,0,0)
for (i in 0:1000) {
  theta[3] <- 1.5 + i*0.005
  logsig[i+1] <- theta[3]
  logsigvar[i+1] <- nll.weibull(theta)
}

plot(logsig, logsigvar, type="l", xlab=expression(log(sigma)), ylab="Variance") #estimate optimal logsigma to be ~2.75


```
```


The function \texttt{nll.weibull} has been optimised in order to obtain the maximum likelihood estimate of the parameter $\boldsymbol{\theta}$, using the built in \texttt{optim} function in $R$. The initial point has been obtained by plotting graphs of each of the parameters versus its variance. The plots have been observed to estimate the value where the variance appears to be lowest. The graphs are displayed below.
```{r,echo=FALSE,message=FALSE, fig.height=2.5}
rats<- read.table("http://people.bath.ac.uk/kai21/ASI/rats_data.txt")
nll.weibull <- function (theta) {
  #create subset of data since for the purposes of calculating the optimal theta since 
  #when status = 0, the survival function does not depend on theta. Need to optimise this 
  #function using optim.
  t1 <- rats[which(rats$status==1),,]$time
  rx1 <- rats[which(rats$status==1),,]$rx
  t0 <- rats[which(rats$status==0),,]$time
  rx0 <- rats[which(rats$status==0),,]$rx
  #compute the negative log likelihood
  k <- 1/exp(theta[3]) #shape 
  lambda <- exp(theta[1] + theta[2]*rx0) #scale with censoring
  loglik1 <- -sum(dweibull(t1, shape=1/exp(theta[3]), scale=exp(theta[1] + theta[2]*rx1), log=TRUE))
  loglik2 <- sum((t0/lambda)^k)
  loglik <- loglik1 + loglik2
  loglik
}
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

par(mfrow=c(1,3))
for (i in 0:1000) {
  theta[1] <- 4.5 + i*0.0025
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.weibull(theta)
}

plot(beta0, beta0var, type="l", xlab=expression(beta[0]), ylab="Variance") 

theta <- c(0,0,0)
for (i in 0:1000) {
  theta[2] <- 4 + i*0.0025
  beta1[i+1] <- theta[2]
  beta1var[i+1] <- nll.weibull(theta)
}

plot(beta1, beta1var, type="l", xlab=expression(beta[1]), ylab="Variance") #estimate optimal beta1 to be ~5.25


theta <- c(0,0,0)
for (i in 0:1000) {
  theta[3] <- 1.5 + i*0.005
  logsig[i+1] <- theta[3]
  logsigvar[i+1] <- nll.weibull(theta)
}

plot(logsig, logsigvar, type="l", xlab=expression(log(sigma)), ylab="Variance") #estimate optimal logsigma to be ~2.75


```

The minimum variance for each of the parameters appears to be at approximately $\boldsymbol{\theta}= (5.75,5.25,2.75)$ and hence this will be taken as the initial point to proceed with the optimization.

The \texttt{optim} function works in such a way that seeks to optimize the given parameters using the given funtion which it will minimize by default. Since the \texttt{nll.weibull} function has been negated, \texttt{optim} will return the maximum likelihood estimate. The \texttt{optim} has been instructed to use the Nelder-Mead optimization method, The optimisation can be seen in the $R$ code below.

```{#numCode .R .numberLines}
theta <- c(5.75,5.25,2.75)
ratlik <- optim(theta, nll.weibull, hessian=TRUE, method="Nelder-Mead")
```

The function \texttt{nll.weibull} has been minimised using optim, hence the log likelihood has been maximised with respect to the initial values of theta $\boldsymbol{\theta}$ using the Nelder-Mead method in \texttt{optim}. The first argument in optim provides the initial parameter values $\boldsymbol{\theta}$ from which to start the optimisation. The next argument is the objective function, \texttt{nll.weibulll}. The first argument of the function \texttt{nll.weibulll} is required to be the vector of parameters which need to be optimised. Hessian=TRUE is an argument passed to \texttt{optim} so that it will return an appropriate hessian matrix at the convergence. The Nelder-Mead method is instructed to be used. It is the default method of optimisation that \texttt{optim} uses.

The returned, objects can be seen below.
\texttt{par}
```{r,echo=FALSE,message=FALSE}
theta <- c(5.75,5.25,2.75)
ratlik <- optim(theta, nll.weibull, hessian=TRUE, method="Nelder-Mead")
weibullapprox <- ratlik$par

#then to find the standard error, take the sqrt of the diag of the inverse hessian
hess <- solve(ratlik$hessian)
stderr <- sqrt(diag(hess))

#95% CI = parameter estimate +- ~1.96*stderr
CI <- c(ratlik$par[2]+qnorm(0.025)*stderr[2], ratlik$par[2]+qnorm(0.975)*stderr[2])
weibullapprox


```
\texttt{value}
```{r,echo=FALSE,message=FALSE}
ratlik$value

```
\texttt{counts}
```{r,echo=FALSE,message=FALSE}
ratlik$counts

```
\texttt{convergence}
```{r,echo=FALSE,message=FALSE}
ratlik$convergence
```
The returned object \texttt{ratlik} is made up of several elements as can be seen above. The \texttt{par} contains the missing parameter values, the maximum likelihood estimates $\boldsymbol{\hat{\theta}}$, \texttt{value} contains the value of the objective function at the maximum and \texttt{counts} indicates how many function evaluations were required in order to terminate the optimization, \texttt{convergence}, when equal to $0$ indicates that the Nelder-Mead method converged at a minimum value for the negative log likelihood. 

The asymptotic standard error of the parameter estimates, $\boldsymbol{\hat{\theta}}$ can be calculated using the hessian. The inverse of the approximated hessian is the asymptotic variance of the MLE and so the standard errors can be calculated by taking the square root of the diagonal elements of the asymptotic variance. This has been executed in $R$ and shown below.

```{#numCode .R .numberLines}
#then to find the standard error, take the sqrt of the diag of the inverse hessian
hess <- solve(ratlik$hessian)
stderr <- sqrt(diag(hess))
```

The $96$% confidence intervals have been calculated in the $R$ code below.

```{#numCode .R .numberLines}
CI <- c(ratlik$par[2]+qnorm(0.025)*stderr[2], ratlik$par[2]+qnorm(0.975)*stderr[2])
LCI <- exp(weibullapprox - 2*stderr)
UCI <- exp(weibullapprox + 2*stderr)
```

The output for the lower (\texttt{LCI}) and upper (\texttt{UCI}) confidence intervals are displayed, respectively, below.

```{r,echo=FALSE,message=FALSE}
LCI <- exp(weibullapprox - 2*stderr)
UCI <- exp(weibullapprox + 2*stderr)
LCI
UCI
```

The interval for $\beta_0$ is relatively wide. The correlation matrix has be computed as using the code below. The interval for $\beta_1$ appears to be relatively wide also.

```{#numCode .R .numberLines}
#estimated correlation matrix of theta hat
corrmat <- diag(1/stderr)%*%hess%*%(diag(1/stderr))
```
\texttt{corrmat} 
```{r,echo=FALSE,message=FALSE}
corrmat <- diag(1/stderr)%*%hess%*%(diag(1/stderr))
corrmat 
```
Looking at \texttt{corrmat} we can see that $\beta_1$ has quite a strong negative correlation of approximately $-0.732$ with $\beta_0$ and $\beta_1$ and $\log(\sigma)$ are moderately correlated with one another with a value of $0.-0.348$. This explains why the confidence intervals for $\beta_0$, are quite wide.

In the context of the study, suppose that we want to test the hypothesis that $\beta_1= 0.7$, the above interval for $\beta_1$ suggests rejecting this hypothesis at the 5% significance level.

The data is then analysed in such a way that the probability model for $T_i$ takes on a log-logistic distribution as seen below. With the same shape and scale parameters as in the Weibull model.

$$ T_i \sim \mbox{log-logistic}(\mbox{shape} = \frac{1}{\sigma}, \mbox{scale} = \exp(\eta_i)),\  \ \ \ \ \  \ \  \eta_i = \beta_0 + \beta_1 x_i $$

Whilst the survival function of a log-logistic distribution can be expressed as.

$$ S(t|\mbox{a,b}) = \exp\bigg(- \log\bigg( 1 + \bigg(\frac{t}{b}\bigg)^a\bigg)\bigg), \  \ \ \ \ \ \ t > 0$$
Hence, the negative log likelihood of the censored observations can be written as below.

$$ (L) _{rx=0}= - \sum \log \bigg( \exp\bigg(-\log \bigg(1 +\bigg(\frac{t_0}{\lambda}\bigg)^k\bigg) \\
=\sum \log \bigg( 1+ \bigg(\frac{t_0}{\lambda}\bigg)^k\bigg), \ \ \ \ \ \ \ \ t_0 > 0 $$ 

In equal fashion to the Weibull distribution, the collective likelihood of both censored and uncensored observations will be written as the sum of $L_{rx=0}$ and $L_{rx=1}$ as they are in terms of logs. The function that will be maximised has been defined in the $R$ code below.

```{#numCode .R .numberLines}
nll.llogistic <- function (theta) {
  #create subsets of data since for the likelihood function 
  #since the contribution to the likelihood varies
  #depending on the 'status' variable
  t0 <- rats[which(rats$status==0),,]$time
  rx0 <- rats[which(rats$status==0),,]$rx
  t1 <- rats[which(rats$status==1),,]$time
  rx1 <- rats[which(rats$status==1),,]$rx
    #compute the negative log likelihood
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1] + theta[2]*rx1) #scale for uncensored obs
  loglik1 <- -sum(log((k/lambda)*(t1/lambda)^(k-1)/(1+(t1/lambda)^k)^2)) 
  #log likelihood of uncensored obs
  lambda <- exp(theta[1] + theta[2]*rx0) 
  #redefine b for new rx vector for censored obs
  loglik2 <- sum(log(1+(t0/lambda)^k)) 
  #log likelihood of censored obs using S(t|a,b)
  loglik <- loglik1 + loglik2 #joint likelihood
  loglik
}
```

The function, \texttt{nll.logistic} that has been defined above will be passed to \texttt{optim} using the initial vector of parameters. Again, the starting point has been estimated in the same fashion as the previous.

```{#numCode .R .numberLines}
#to pick initial point, vary paramaters of theta individually  
#and fix the others to find a rough minimum

#initialize variables
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

for (i in 0:1000) {
  theta[1] <- 2.5 + i*0.005
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.llogistic(theta)
}
plot(beta0, beta0var, type="l") #estimate optimal beta0 to be ~ 5.75

theta <- c(5.75,0,0)
for (i in 0:1000) {
  theta[2] <- -2.5 + i*0.005
  beta1[i+1] <- theta[2]
  beta1var[i+1] <- nll.llogistic(theta)
}
plot(beta1, beta1var, type="l") #estimate optimal beta1 to be ~ -0.5

theta <- c(5.75,-0.5,0)
for (i in 0:1000) {
  theta[3] <- -2.5 + i*0.005
  logsig[i+1] <- theta[3]
  logsigvar[i+1] <- nll.llogistic(theta)
}
plot(logsig, logsigvar, type="l") #estimate optimal logsigma to be ~ -0.5

```

The plots created by the code can be seen below.

```{r,echo=FALSE,message=FALSE, fig.height=2.5}
nll.llogistic <- function (theta) {
  #initialise variables from dataset
  t <- rats$time
  s <- rats$status
  rx <- rats$rx
  #define shape and scale of log-logistic distribution
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1] + theta[2]*rx) #scale
  #compute negative log likelihood
  loglik <- -sum(log(((k/lambda)*(t/lambda)^(k-1)/(1+(t/lambda)^k)^2)^s*(1/(1+(t/lambda)^k))^(1-s)))
  loglik
}
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

for (i in 0:1000) {
  theta[1] <- 2.5 + i*0.005
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.llogistic(theta)
}
#plot(beta0, beta0var, type="l") #estimate optimal beta0 to be ~ 5.75

theta <- c(5.75,0,0)
for (i in 0:1000) {
  theta[2] <- -2.5 + i*0.005
  beta1[i+1] <- theta[2]
  beta1var[i+1] <- nll.llogistic(theta)
}
#plot(beta1, beta1var, type="l") #estimate optimal beta1 to be ~ -0.5

theta <- c(5.75,-0.5,0)
for (i in 0:1000) {
  theta[3] <- -2.5 + i*0.005
  logsig[i+1] <- theta[3]
  logsigvar[i+1] <- nll.llogistic(theta)
}
#plot(logsig, logsigvar, type="l") #estimate optimal logsigma to be ~ -0.5
par(mfrow=c(1,3))
plot(beta0, beta0var, type="l", xlab=expression(beta[0]), ylab="Variance") #estimate optimal beta0 to be ~5.75
plot(beta1, beta1var, type="l", xlab=expression(beta[1]), ylab="Variance") #estimate optimal beta1 to be ~-0.5
plot(logsig, logsigvar, type="l", xlab=expression(log(sigma)), ylab="Variance") #estimate optimal logsigma to be ~-0.5
```
The observed minima on the graphs, will again be used as to estimate the starting parameters. This appears to be approximately at $\boldsymbol{\theta} = ( 5.75, -0.5, -0.5)$. Subsequent to this, the function will be optimized with respect to the chosen initial point $\boldsymbol{\theta}$, the $R$ code written to achieve this is displayed below.  

```{#numCode .R .numberLines}
theta <- c(5.75,-0.5,-0.5)
nll.optim <- optim(theta, nll.llogistic, hessian=TRUE, method="BFGS")

```
The objects returned by \texttt{nll.optim} can be seen as below.   
\texttt{par}
```{r,echo=FALSE,message=FALSE}
nll.llogistic <- function (theta) {
  #create subsets of data since for the likelihood function 
  #since the contribution to the likelihood varies
  #depending on the 'status' variable
  t0 <- rats[which(rats$status==0),,]$time
  rx0 <- rats[which(rats$status==0),,]$rx
  t1 <- rats[which(rats$status==1),,]$time
  rx1 <- rats[which(rats$status==1),,]$rx
    #compute the negative log likelihood
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1] + theta[2]*rx1) #scale for uncensored obs
  loglik1 <- -sum(log((k/lambda)*(t1/lambda)^(k-1)/(1+(t1/lambda)^k)^2)) 
  #log likelihood of uncensored obs
  lambda <- exp(theta[1] + theta[2]*rx0) 
  #redefine b for new rx vector for censored obs
  loglik2 <- sum(log(1+(t0/lambda)^k)) 
  #log likelihood of censored obs using S(t|a,b)
  loglik <- loglik1 + loglik2 #joint likelihood
  loglik
}
theta <- c(5.75,-0.5,-0.5)
nll.optim <- optim(theta, nll.llogistic, hessian=TRUE, method="BFGS")
theta.llog <- nll.optim$par 
theta.llog 
```
\texttt{value}
```{r,echo=FALSE,message=FALSE}
theta.llog <- nll.optim$value 
theta.llog 
```
\texttt{counts}
```{r,echo=FALSE,message=FALSE}
theta.llog <- nll.optim$counts 
theta.llog 
```
\texttt{convergence}
```{r,echo=FALSE,message=FALSE}
theta.llog <- nll.optim$convergence 
theta.llog 
```
The returned object \texttt{nll.optim} is made up of several elements. The output \texttt{par} contains the returned optimized parameter values, $\boldsymbol{\tilde{\theta}}$ following a log-logistic distribution. The output, \texttt{value} contains the maximum likelihood estimate for the function at initial $\boldsymbol{\theta}$. It can be seen that from the \texttt{counts} output, that $35$ function evaluations and $12$ gradient evalutions were required in order to converge to the optimal solution in this case, whist \texttt{convergence} equal to $0$ confirms that the solution converged.

It is now left to estimate the asymptotoic standard errors of each parameter estimate. This along with a $95$% confidence interval is estimated using $R$ code as below.

```{#numCode .R .numberLines}
#name the nll.optim hessian
hess <- nll.optim$hessian
#to find the standard error, take the sqrt of the diag of the inverse hessian
se <- sqrt(diag(solve(hess)))
#95% CI = parameter estimate +- ~1.96*stderr
CI <- c(theta.llog[2] + qnorm(0.025)*se[2], theta.llog[2] + qnorm(0.975)*se[2])
```
The asymptotic standard error of each of the parameter estimates $\boldsymbol{\tilde{\theta}}$ is shown as below. 
```{r,echo=FALSE,message=FALSE}
se <- sqrt(diag(solve(hess)))
se

```

It can be seen that the asymptotic standard error of $\beta_1$ has been approximated at a value of $17.36748$.

Following this, the Weibull distribution was again considered, but with added random effect describing the litter, $\boldsymbol{b}$, taken into consideration. So it can be seen that the scale of the Weibull distribution is then altered to the exponent of $\eta_i$. Where $\eta_i$ is now as follows:
$$ \eta_i = \beta_0 + \beta_1 x_i + b_{litter(i)} $$

Primarily, a function \texttt{lfyb} has been created, which evaluates the joint log density of the observations $\boldsymbols{y}$ and the random effect $\boldsymbol{b}$. It is also necessary to consider that the function will also be required to be suitable for optimizing. The arguments of the function will be $/boldsymbol{\theta}$, $\boldsymbol{y}$, $\boldsymbol{b}$, $X$ and $Z$. The following $R$ code is a means of constructing these arguments.

```{#numCode .R .numberLines}
#theta = [beta0, beta1, log(sigma), log(sigmab)]
X <- model.matrix(~ rx + status, rats) #define X matrix
rats$litter <- factor(rats$litter)
Z <- model.matrix(~ litter - 1, rats) #define Z matrix
b <- rnorm(50,0,0.1) #set arbitrary b
y <- rats$time #define y vector
theta <- c(4.9831505, -0.2384417, -1.3324342, 0) #using optimal paramater estimates from Q1, & small log(sig.b)
```

These arguments are then used to construct the function. The $R$ code below displays the function.
```{#numCode .R .numberLines}
lfyb <- function (b, y, theta, X, Z) {
  #function to compute the joint log density of y and b, which is the
  #(conditional log likelihood of y given b) + (likelihood of b)
  #first compute conditional density of y given b
  s <- X[,3] #status
  beta <- c(theta[1:2],0) #define beta vector with a 0 in the last position since status is not relevant for eta
  eta <- as.numeric(X%*%beta + Z%*%b) #define eta vector
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(eta) #scale vector
  #log conditional likelihood of y given b
  nll1 <- k*log(lambda) - log(k) - (k-1)*log(y) + (y/lambda)^k #scale = 1
  nll0 <- (y/lambda)^k #scale = 0
  lfy_b <- sum(s*nll1 + (1-s)*nll0)
  #negative log likelihood of b
  sig.b <- exp(theta[4])
  #lfb <- -sum(dnorm(b, 0, sig.b, log=TRUE))
  lfb <- dnorm(b, 0, sig.b)
  lfb <- log(lfb)
  lfb[lfb==-Inf] <- -10
  lfb <- -sum(lfb)
  #compute and output negative joint log density
  lf <- lfy_b + lfb
  lf
}

```

It then follows to optimize the function \texttt{lal} for given parameters $\boldsymbol{\theta}$. This has been done using he $R$ code below and the returned objects displayed beneath.

```{#numCode .R .numberLines}
  lal.opt <- optim(theta, lal, y=y, X=X, Z=Z, method="BFGS", hessian=TRUE)
```

```{r,echo=FALSE,message=FALSE}
  lal.opt <- optim(theta, lal, y=y, X=X, Z=Z, method="BFGS", hessian=TRUE)
  lal.opt

```

The parameter values for \texttt{lal.opt}


As a means of estimating the relatively quality amongst the different maximum likelihood estimates, the AIC has been calculated for each of the models using the following code.

```{#numCode .R .numberLines}
##To calculate the AIC
aic.rand <- 2*lal.opt$value + 2*length(lal.opt$par)
aic.weibull <-2*weibull.optim$value + 2*length(weibull.optim$par)
aic.nll <-2*nll.optim$value + 2*length(nll.optim$par)

```

The values for the AIC are not very dispersed so relative to one another, the models relative quality of statistical models for the given set of data \texttt{rats} are fairly equal. The lowest being the Laplaces approximation with the random effects considered at $490.7679$ , whilst the highest is the negative log likelihood model with AIC at $492.7679$. This might suggest that the Laplaces 

