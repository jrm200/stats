---
title: "MA40198: Applied Statistical Inference"
author: "Candidates: 07604, 07614, 07615"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Carcinogenesis Study on Rats

### Introduction

The recorded data includes the variables:
* \texttt{litter} comprised of three rats pertaining to fifty different litters, 
* \texttt{rx} when $rx=1$ indicates the treatment was carried out on the rat and where $rx=0$ indicates the rat received the control/placebo only,
* \texttt{time} the follow up time, in weeks, to the appearance of a tumor,
* \texttt{status} status$=1$ describes that the presence of a tumor and status$=0$ implies the rat died before the presence of a tumor.

The drug trial was carried out on solely female rats in a controlled environment and saw one in three of the rats per litter being treated for cancer whilst the other two received a placebo treatment only. Following the treatment, the rats were detained and observed. If a tumor appeared on a rat or if the rat died, the occurance was recorded at the specific time in weeks, indicating which of the two outcomes occurred. This drug trial was carried out with the motivation to discover whether or not the method of cancer treatment prolonged the life of those that were treated.

Using Maximum Likelihood Estimation, this study is comprised of frequentist estimation procedures and Bayesian estimation procedures.

### Methods

Upon the first approach to the MLE problem, the dependent variable time described by $T_i$, has been taken to follow a Weibull distribution as we cannot assume that the data does not complies with the assumptions of normality. The distribution is as below.
$$ T_i \sim \mbox{Weibull}(\mbox{shape} = \frac{1}{\sigma}, \mbox{scale}=\exp(\eta_i)), \ \ \ \ \ \ \ \ \eta_i=\beta_0 + \beta_1 x_i $$
where
$$x_i=\left\{
\begin{array}{cl}
1 & \mbox{rat $i$ received treatment} \\
0 & \mbox{rat $i$ received control}\\
\end{array}
\right.$$ 
It can be seen from the above that the shape parameter in the model takes the form of $\frac{1}{\sigma}$ whilst the scale parameter is proportional to $\exp(\beta_0 + \beta_1 x_i)$. Censored observations, where the Treatment Indicator, $rx=0$, implied that the rat received only the control, are described by the survival function of a Weibull distribution given as follows.
$$ S(t|\mbox{a,b})= \exp\bigg(-\bigg( \frac{t}{b}\bigg)^a\bigg), \ \ \ \ \ \ \ \ t> 0 $$ 
The maximum likelihood estimate calculates an estimation of the population parameters such that the likelihood of obtaining the observed data is maximised. The vector of parameters to be estimated is $\boldsymbol{\theta}^T=( \beta_0, \beta_1, \log(\sigma))$.

Given all observations $t_i$ are independent of one another, which we can assume to be true, the likelihood of the whole sample $T$ is the product of the individual likelihoods over all observations $t_i$, where $L$ is the likelihood. L can be defined as follows.

$$ L=L_1 \times L_2 \times ... \times L_N = \prod^N_{i=1}{L_i} = P(y_1| \boldsymbol{\hat{\theta}}) \times P(y_2| \boldsymbol{\hat{\theta}}) \times ... \times P(y_N| \boldsymbol{\hat{\theta}}) = \prod^N_{i=1} P(y_i|\boldsymbol{\hat{\theta}} ) $$
It is true that the likelihood will always be positive and since the $\log$ function is a non decreasing function, if we transform the likelihood by taking the $\log$ of it, the maxima will remain in the same position. When working with $\log$ it is more versatile as the sum of the log of the likelihood can be taken.

A function to describe the likelihood of the parameters $\boldsymbol{\theta}$ is defined in $R$ as below.

```{R }
rats<- read.table("http://people.bath.ac.uk/kai21/ASI/rats_data.txt")
#define negative log likelihood function to minimise
nll.weibull <- function (theta) {
  #initialise variables from dataset
  s <- rats$status
  rx <- rats$rx
  t <- rats$time
  #define shape and scale of Weibull distribution
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1] + theta[2]*rx) #scale
  #compute negative log likelihood
  loglik <- -sum(log(((k/lambda)*(t/lambda)^(k-1)*exp(-(t/lambda)^k))^s*(exp(-(t/lambda)^k))^(1-s)))
  loglik
}
```


Initially, the function describing the negative log likelihood that will be maximised has been defined as \texttt{nll.weibull}. This function includes subsetted data, so that the optimal $\boldsymbol{\theta}$ can be calculated considering the censored and uncensored observations.

In order to compute the log likelihood for the censored observations, the shape and scale parameters of the survival function have been defined below. Given that the third element in $\boldsymbol{\theta}$ is $\log(\sigma)$, the exponent of $\sigma$ has been taken.
$$ \mbox{Shape}=\frac{1}{\exp(\sigma)}, \ \ \ \ \ \mbox{Scale} = \exp(\beta_0 + \beta_1rx_0) $$
The scale is altered by $rx_0$, since we are considering the treatment indicator based on only censored observations. The shape can be referred to as $k$ whilst the scale can be referred to as $\lambda$. 

Since the method of finding the MLE has utilised the $\texttt{optim}$ function in $R$, the negative log likelihoods are computed, as $\texttt{optim}$ optimises the given function and seeks the minimum value. The likelihood for censored observations has been defined below.

$$ (L) _{rx=0}= - \sum -\log\bigg(\exp\bigg(-\bigg(\frac{t_0}{\lambda}\bigg)^k\bigg) \\
=\sum \bigg(\frac{t_0}{\lambda}\bigg)^k, \ \ \ \ \ \ \ \ t_0 > 0 $$ 

In order to compute the likelihood of the observations in which the appearance of a tumor occured, the shape and scale parameters define below were passed to the built in $R$ function \texttt{dweibull}.

$$ \mbox{shape}= \frac{1}{\exp(\sigma)}, \ \ \ \ \ \ \ \ \mbox{scale}=\exp(\beta_0 + \beta_{1} rx_{1}) $$ 

The following code demonstrates how a plot was created for $\beta_0$ in order to pick an appropriate initial point to use when optimising \texttt{weibull}.

```{R }
#to pick initial theta, vary paramaters of theta individually and fix the others to find an approximate minimum
#initialize variables
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

for (i in 0:1000) {
  theta[1] <- 4.5 + i*0.0025
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.weibull(theta)
}

plot(beta0, beta0var, type="l") #estimate optimal beta0 to be ~5.75

theta <- c(0,0,0)
for (i in 0:1000) {
  theta[2] <- 4 + i*0.0025
  beta1[i+1] <- theta[2]
  beta1var[i+1] <- nll.weibull(theta)
}

```

These plots were created for each of the parameters of $\boldsymbol{\theta}$. The initial point has been obtained by plotting graphs of each of the parameters versus its variance. The plots have been observed to estimate the value where the variance appears to be lowest. The graphs are displayed below.
```{r,echo=FALSE,message=FALSE, fig.height=2.5}
rats<- read.table("http://people.bath.ac.uk/kai21/ASI/rats_data.txt")
nll.weibull <- function (theta) {
  #create subset of data since for the purposes of calculating the optimal theta since 
  #when status = 0, the survival function does not depend on theta. Need to optimise this 
  #function using optim.
  t1 <- rats[which(rats$status==1),,]$time
  rx1 <- rats[which(rats$status==1),,]$rx
  t0 <- rats[which(rats$status==0),,]$time
  rx0 <- rats[which(rats$status==0),,]$rx
  #compute the negative log likelihood
  k <- 1/exp(theta[3]) #shape 
  lambda <- exp(theta[1] + theta[2]*rx0) #scale with censoring
  loglik1 <- -sum(dweibull(t1, shape=1/exp(theta[3]), scale=exp(theta[1] + theta[2]*rx1), log=TRUE))
  loglik2 <- sum((t0/lambda)^k)
  loglik <- loglik1 + loglik2
  loglik
}
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

par(mfrow=c(1,3))
for (i in 0:1000) {
  theta[1] <- 4.5 + i*0.0025
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.weibull(theta)
}

plot(beta0, beta0var, type="l", xlab=expression(beta[0]), ylab="Variance") 

theta <- c(0,0,0)
for (i in 0:1000) {
  theta[2] <- 4 + i*0.0025
  beta1[i+1] <- theta[2]
  beta1var[i+1] <- nll.weibull(theta)
}

plot(beta1, beta1var, type="l", xlab=expression(beta[1]), ylab="Variance") #estimate optimal beta1 to be ~5.25


theta <- c(0,0,0)
for (i in 0:1000) {
  theta[3] <- 1.5 + i*0.005
  logsig[i+1] <- theta[3]
  logsigvar[i+1] <- nll.weibull(theta)
}

plot(logsig, logsigvar, type="l", xlab=expression(log(sigma)), ylab="Variance") #estimate optimal logsigma to be ~2.75


```

The minimum variance for each of the parameters appears to be at approximately $\boldsymbol{\theta}= (5.75,5.25,2.75)$ and hence this will be taken as the initial point to proceed with the optimization.

\texttt{nll.weibull} wil be optimised in order to obtain the MLE of the parameters $\boldsymbol{\theta}$, using \texttt{optim}. The \texttt{optim} function works in such a way that seeks to optimize the given parameters using the given funtion which it will minimize by default. Since the \texttt{nll.weibull} function has been negated, \texttt{optim} will return the maximum likelihood estimate. \texttt{optim} has been instructed to use the Nelder-Mead optimization method as seen below.

```{R}
theta <- c(5.75,5.25,2.75)
weibull.optim <- optim(theta, nll.weibull, method="BFGS", hessian=TRUE)
```

The function \texttt{nll.weibull} has been minimised using \texttt{optim}, hence the log likelihood has been maximised with respect to the initial values of theta $\boldsymbol{\theta}$ using the Nelder-Mead method in \texttt{optim}. The first argument in \texttt{optim} provides the initial parameter values $\boldsymbol{\theta}$ from which to start the optimisation. The next argument is the objective function, \texttt{nll.weibulll}. The first argument of the function \texttt{nll.weibulll} is required to be the vector of parameters which need to be optimised. \texttt{optim} has been instructed to return an approximate Hessian matrix at the convergence.

The returned objects are presented below.   
\texttt{par}
```{r}
theta <- c(5.75,5.25,2.75)
weibull.optim <- optim(theta, nll.weibull, method="BFGS", hessian=TRUE)
theta.weibull <- weibull.optim$par
hess <- solve(weibull.optim$hessian)

#then to find the standard error, take the square root of the diagonal of the inverse hessian
se <- sqrt(diag(hess))

#95% CI = parameter estimate +- ~1.96*stderr
CI.norand <- c(theta.weibull[2]+qnorm(0.025)*se[2], theta.weibull[2]+qnorm(0.975)*se[2])
par
```
\texttt{value}
```{r,echo=FALSE,message=FALSE}
weibull.optim$value

```
\texttt{counts}
```{r,echo=FALSE,message=FALSE}
weibull.optim$counts

```
\texttt{convergence}
```{r,echo=FALSE,message=FALSE}
weibull.optim$convergence
```

\texttt{par} contains the parameter values, the maximum likelihood estimates $\boldsymbol{\hat{\theta}}$. \texttt{value} contains the value of the objective function at the maximum and \texttt{counts} indicates how many function evaluations were required in order to terminate the optimization. \texttt{convergence} when equal to $0$ indicates that the \texttt{optim} method utilising BFGS converged at a minimum value for the negative log likelihood. 

The asymptotic standard error of the parameter estimates is calculated using the Hessian. The inverse of the approximated hessian is the asymptotic variance of the MLE and so the standard errors can be calculated by taking the square root of the diagonal elements of the asymptotic variance. This has been executed in $R$ and shown below.

```{R}
#then to find the standard error, take the sqrt of the diag of the inverse hessian
hess <- solve(weibull.optim$hessian)
stderr <- sqrt(diag(hess))
```

The $95%$ confidence interval for the paramater $\beta_1$ have been calculated with the code below.

```{R}
CI.norand <- c(theta.weibull[2]+qnorm(0.025)*se[2], theta.weibull[2]+qnorm(0.975)*se[2])
```

The $95%$ asymptotic confidence interval for $\beta_1$ is displayed below along with the standard error estimates.

```{r,echo=FALSE,message=FALSE}
hess <- solve(weibull.optim$hessian)
stderr <- sqrt(diag(hess))
CI.norand <- c(theta.weibull[2]+qnorm(0.025)*se[2], theta.weibull[2]+qnorm(0.975)*se[2])
CI.norand
stderr
```

The correlation matrix has been created using the following code.

```{R}
#estimated correlation matrix of theta hat
corrmat <- diag(1/stderr)%*%hess%*%(diag(1/stderr))
```
\texttt{corrmat} 
```{r,echo=FALSE,message=FALSE}
corrmat <- diag(1/stderr)%*%hess%*%(diag(1/stderr))
corrmat 
```
Looking at the correlation matrix we can see that $\beta_1$ has quite a strong negative correlation of approximately $-0.732$ with $\beta_0$ and $\beta_1$ and $\log(\sigma)$ are moderately correlated with one another with a value of $-0.348$. The optimal parameter estimate for $\beta_1$ has been approximated at approximately $-0.2384417$ and the $95%$ asymptotic confidence interval for this value is ranging over the values $(-0.41311130, -0.0639883)$. This interval seems relatively wide considering the context of the study. For example, the absolute value of this range can be approximated at $0.34960247$ which is greater than the magnitude of the parameter estimate itself. Since $\beta_1$ directly effects the \texttt{status} this suggests that any estimate for $\beta_1$ falling within this range has a 95% certainty that it fits the true population.

The data will now be analysed on the the probability model for $T_i$ takes on a log-logistic distribution as seen below with the same shape and scale parameters as in the Weibull model.

$$ T_i \sim \mbox{log-logistic}(\mbox{shape} = \frac{1}{\sigma}, \mbox{scale} = \exp(\eta_i)),\  \ \ \ \ \  \ \  \eta_i = \beta_0 + \beta_1 x_i $$

Whilst the survival function of a log-logistic distribution can be expressed as.

$$ S(t|\mbox{a,b}) = \exp\bigg(- \log\bigg( 1 + \bigg(\frac{t}{b}\bigg)^a\bigg)\bigg), \  \ \ \ \ \ \ t > 0$$
Hence, the negative log likelihood of the censored observations can be written as below.

$$ (L) _{rx=0}= - \sum \log \bigg( \exp\bigg(-\log \bigg(1 +\bigg(\frac{t_0}{\lambda}\bigg)^k\bigg) \\
=\sum \log \bigg( 1+ \bigg(\frac{t_0}{\lambda}\bigg)^k\bigg), \ \ \ \ \ \ \ \ t_0 > 0 $$ 

In equal fashion to the Weibull distribution, the collective likelihood of both censored and uncensored observations will be written as the sum of $L_{rx=0}$ and $L_{rx=1}$ as they are in terms of logs. The function that will be minimised has been defined in the $R$ code below.

```{R}
#define negative log likelihood function to minimise
nll.llogistic <- function (theta) {
  #initialise variables from dataset
  t <- rats$time
  s <- rats$status
  rx <- rats$rx
  #define shape and scale of log-logistic distribution
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1] + theta[2]*rx) #scale
  #compute negative log likelihood
  loglik <- -sum(log(((k/lambda)*(t/lambda)^(k-1)/(1+(t/lambda)^k)^2)^s*(1/(1+(t/lambda)^k))^(1-s)))
  loglik
}
```

The function, \texttt{nll.logistic} that has been defined above will be passed to \texttt{optim} using the initial vector of parameters. Again, the starting point has been estimated in the same fashion as the previous and the code below demonstrates this method for the parameter $\beta_0$.

```{R}
#to pick initial point, vary paramaters of theta individually  
#and fix the others to find a rough minimum

#initialize variables
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

for (i in 0:1000) {
  theta[1] <- 2.5 + i*0.005
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.llogistic(theta)
}
plot(beta0, beta0var, type="l") #estimate optimal beta0 to be ~ 5.75


```

The plots for each parameter can be seen below.

```{r,echo=FALSE,message=FALSE, fig.height=2.5}
nll.llogistic <- function (theta) {
  #initialise variables from dataset
  t <- rats$time
  s <- rats$status
  rx <- rats$rx
  #define shape and scale of log-logistic distribution
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1] + theta[2]*rx) #scale
  #compute negative log likelihood
  loglik <- -sum(log(((k/lambda)*(t/lambda)^(k-1)/(1+(t/lambda)^k)^2)^s*(1/(1+(t/lambda)^k))^(1-s)))
  loglik
}
theta <- c(0,0,0)
beta0 <- theta[1]
beta1 <- theta[2]
logsig <- theta[3]
beta0var <- 0
beta1var <- 0
logsigvar <- 0

for (i in 0:1000) {
  theta[1] <- 2.5 + i*0.005
  beta0[i+1] <- theta[1]
  beta0var[i+1] <- nll.llogistic(theta)
}
#plot(beta0, beta0var, type="l") #estimate optimal beta0 to be ~ 5.75

theta <- c(5.75,0,0)
for (i in 0:1000) {
  theta[2] <- -2.5 + i*0.005
  beta1[i+1] <- theta[2]
  beta1var[i+1] <- nll.llogistic(theta)
}
#plot(beta1, beta1var, type="l") #estimate optimal beta1 to be ~ -0.5

theta <- c(5.75,-0.5,0)
for (i in 0:1000) {
  theta[3] <- -2.5 + i*0.005
  logsig[i+1] <- theta[3]
  logsigvar[i+1] <- nll.llogistic(theta)
}
#plot(logsig, logsigvar, type="l") #estimate optimal logsigma to be ~ -0.5
par(mfrow=c(1,3))
plot(beta0, beta0var, type="l", xlab=expression(beta[0]), ylab="Variance") #estimate optimal beta0 to be ~5.75
plot(beta1, beta1var, type="l", xlab=expression(beta[1]), ylab="Variance") #estimate optimal beta1 to be ~-0.5
plot(logsig, logsigvar, type="l", xlab=expression(log(sigma)), ylab="Variance") #estimate optimal logsigma to be ~-0.5
```
The observed minima on the graphs will again be used as to estimate the starting parameters. This appears to be approximately at $\boldsymbol{\theta} = ( 5.75, -0.5, -0.5)$. Subsequent to this, the function will be optimized with respect to the chosen initial point $\boldsymbol{\theta}$. The $R$ code constructed in order to achieve this is below.  

```{R}
theta <- c(5.75,-0.5,-0.5)
nll.optim <- optim(theta, nll.llogistic, hessian=TRUE, method="BFGS")

```
The objects returned by \texttt{nll.optim} can be seen below.   
\texttt{par}
```{r,echo=FALSE,message=FALSE}
nll.llogistic <- function (theta) {
  #create subsets of data since for the likelihood function 
  #since the contribution to the likelihood varies
  #depending on the 'status' variable
  t0 <- rats[which(rats$status==0),,]$time
  rx0 <- rats[which(rats$status==0),,]$rx
  t1 <- rats[which(rats$status==1),,]$time
  rx1 <- rats[which(rats$status==1),,]$rx
    #compute the negative log likelihood
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(theta[1] + theta[2]*rx1) #scale for uncensored obs
  loglik1 <- -sum(log((k/lambda)*(t1/lambda)^(k-1)/(1+(t1/lambda)^k)^2)) 
  #log likelihood of uncensored obs
  lambda <- exp(theta[1] + theta[2]*rx0) 
  #redefine b for new rx vector for censored obs
  loglik2 <- sum(log(1+(t0/lambda)^k)) 
  #log likelihood of censored obs using S(t|a,b)
  loglik <- loglik1 + loglik2 #joint likelihood
  loglik
}
theta <- c(5.75,-0.5,-0.5)
nll.optim <- optim(theta, nll.llogistic, hessian=TRUE, method="BFGS")
theta.llog <- nll.optim$par 
theta.llog 
```
\texttt{value}
```{r,echo=FALSE,message=FALSE}
theta.llog <- nll.optim$value 
theta.llog 
```
\texttt{counts}
```{r,echo=FALSE,message=FALSE}
theta.llog <- nll.optim$counts 
theta.llog 
```
\texttt{convergence}
```{r,echo=FALSE,message=FALSE}
theta.llog <- nll.optim$convergence 
theta.llog 
```
It can be seen that from the \texttt{counts} output, that $35$ function evaluations and $12$ gradient evalutions were required in order to converge to the optimal solution in this case. \texttt{convergence} equal to $0$ confirms that the solution converged at the optimal.

It is now left to estimate the asymptotoic standard errors of each parameter estimate. This along with a $95%$ confidence interval is estimated using the $R$ code below.

```{R}
#name the nll.optim hessian
hess <- nll.optim$hessian
#to find the standard error, take the sqrt of the diag of the inverse hessian
se <- sqrt(diag(solve(hess)))
#95% CI = parameter estimate +- ~1.96*stderr
CI <- c(theta.llog[2] + qnorm(0.025)*se[2], theta.llog[2] + qnorm(0.975)*se[2])
```
The asymptotic standard error of each of the parameter estimates $\boldsymbol{\hat{\theta}}$, along with the $95%$ asymptotic confidence interval for the treatment effect $\beta_1$. 
```{r,echo=FALSE,message=FALSE}
se <- sqrt(diag(solve(hess)))
se
CI
```

It can be seen that the asymptotic standard error of $\beta_1$ has been approximated at a value of $17.36748$. Whilst the $95%$ confidence interval for $\beta_1$ is computed as $(-0.41689587, -0.04212134)$. Comparatively to the MLE of the Weibull distribution, this confidence interval has increased slightly in magnitude. Hence there is a greater range of values that could be taken to be the parameter estimate, such that the true mean of the population is with $95%$ certainty.

Proceeding onward, the Weibull distribution was again considered, but with added random effect describing the litter, $\boldsymbol{b}$. So it can be seen that the scale of the Weibull distribution is then altered to the exponent of $\eta_i$. Where $\eta_i$ is now as follows:
$$ \eta_i = \beta_0 + \beta_1 x_i + b_{litter(i)} $$

In order to obtain the likelihood of such a distribution given the data and the added random effect, $\boldsymbol{b}$, the integral must be found as well as the joint density. In order to surpass the intractable integral, we use the Laplace approximation to obtain the likelihood.

So that the Laplace approximation can be utilised to obtain the likelihood, a function \texttt{lfyb} has been created. This evaluates the negative joint log density of the observations $\boldsymbol{y}$ and the added random effect, _litter_ $\boldsymbol{b}$ given the initial vector of parameters $\boldsymbol{\theta}$. It then follows that in order to use the Laplace approximation for the likelihood, the minimum of the negative log joint density of the random effect $\boldsymbol{b}$ and the data $\boldsymbol{y}$ as well as the relevant Hessian must be obtained.

Hence, it is necessary to create the function \texttt{lfyb} such that the arguments of the function will be $\boldsymbol{\theta}$, $\boldsymbol{y}$, $\boldsymbol{b}$, $X$ and $Z$. The following $R$ code is a means of constructing these arguments.

```{R}
#theta = [beta0, beta1, log(sigma), log(sigmab)]
X <- model.matrix(~ rx + status, rats) #define X matrix
rats$litter <- factor(rats$litter)
Z <- model.matrix(~ litter - 1, rats) #define Z matrix
b <- rnorm(50,0,0.1) #set arbitrary b
y <- rats$time #define y vector
theta <- c(4.9831505, -0.2384417, -1.3324342, 0) #using optimal paramater estimates as initial from Q1, & small log(sig.b)
```

These arguments are then used to construct the function. \texttt{lfyb} as such:

```{R}
lfyb <- function (b, y, theta, X, Z) {
  #function to compute the joint log density of y and b, which is the
  #(conditional log likelihood of y given b) + (likelihood of b)
  #first compute conditional density of y given b
  s <- X[,3] #status
  beta <- c(theta[1:2],0) #define beta vector with a 0 in the last position since status is not relevant for eta
  eta <- as.numeric(X%*%beta + Z%*%b) #define eta vector
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(eta) #scale vector
  #log conditional likelihood of y given b
  nll1 <- k*log(lambda) - log(k) - (k-1)*log(y) + (y/lambda)^k #scale = 1
  nll0 <- (y/lambda)^k #scale = 0
  lfy_b <- sum(s*nll1 + (1-s)*nll0)
  #negative log likelihood of b
  sig.b <- exp(theta[4])
  #lfb <- -sum(dnorm(b, 0, sig.b, log=TRUE))
  lfb <- dnorm(b, 0, sig.b)
  lfb <- log(lfb)
  lfb[lfb==-Inf] <- -10
  lfb <- -sum(lfb)
  #compute and output negative joint log density
  lf <- lfy_b + lfb
  lf
}

```

Following this, the gradient has been computed numerically using the following $R$ code.

```{R}
#compute the derivative for status = 1
expr1 <- expression(k*(B0 + B1*rx + b.aux) + (y/exp(B0 + B1*rx + b.aux))^k)
nll.aux1 <- deriv(expr1, c("b.aux"), function.arg=c("b.aux", "B0", "B1", "k", "rx", "y"))

#status = 0
expr0 <- expression((y/exp(B0 + B1*rx + b.aux))^k)
nll.aux0 <- deriv(expr0, c("b.aux"), function.arg=c("b.aux", "B0", "B1", "k", "rx", "y"))

#numerically compute the gradient of lfyb
grad <- function (b, y, theta, X, Z) {
  #define variables
  rx <- X[,2]
  s <- X[,3]
  B0 <- theta[1]
  B1 <- theta[2]
  k <- 1/exp(theta[3])
  b.aux <- Z%*%b
  #gradient if status = 1
  b.grad1 <- nll.aux1(b.aux, B0, B1, k, rx, y)
  g1 <- attr(b.grad1, "gradient")
  g1 <- as.numeric(g1)
  #gradient if status = 0
  b.grad0 <- nll.aux0(b.aux, B0, B1, k, rx, y)
  g0 <- attr(b.grad0, "gradient")
  g0 <- as.numeric(g0)
  #select contribution to gradient based on status
  g <- s*g1 +(1-s)*g0
  #gradient of negative log likelihood of b
  b.grad <- b/exp(2*theta[4])
  m <- matrix(g, nrow=3, ncol=50)
  g <- colSums(m) + b.grad #add contributions from b terms
  g
}

```

It then follows to create a function \texttt{lal} for given parameters $\boldsymbol{\theta}$. We can optimize $\boldsymbol{b}$ within \texttt{lal}. This function takes the optimal values for $\boldsymbol{b}$, $\boldsymbol{\hat{b}}$ to then find the Laplace approximation for $\boldsymbol{\hat{\theta}}$, given $\boldsymbol{\hat{b}}$. This has been done using the $R$ code below.

```{r,echo=FALSE,message=FALSE}
rats<- read.table("http://people.bath.ac.uk/kai21/ASI/rats_data.txt")

#theta = [beta0, beta1, log(sigma), log(sigmab)]
X <- model.matrix(~ rx + status, rats) #define X matrix
rats$litter <- factor(rats$litter)
Z <- model.matrix(~ litter - 1, rats) #define Z matrix
b <- rnorm(50,0,0.1) #set arbitrary b
y <- rats$time #define y vector
theta <- c(4.9831505, -0.2384417, -1.3324342, 0) #using optimal paramater estimates from Q1, & small log(sig.b)

lfyb <- function (b, y, theta, X, Z) {
  #function to compute the joint log density of y and b, which is the
  #(conditional log likelihood of y given b) + (likelihood of b)
  #first compute conditional density of y given b
  s <- X[,3] #status
  beta <- c(theta[1:2],0) #define beta vector with a 0 in the last position since status is not relevant for eta
  eta <- as.numeric(X%*%beta + Z%*%b) #define eta vector
  k <- 1/exp(theta[3]) #shape
  lambda <- exp(eta) #scale vector
  #log conditional likelihood of y given b
  nll1 <- k*log(lambda) - log(k) - (k-1)*log(y) + (y/lambda)^k #scale = 1
  nll0 <- (y/lambda)^k #scale = 0
  lfy_b <- sum(s*nll1 + (1-s)*nll0)
  #negative log likelihood of b
  sig.b <- exp(theta[4])
  #lfb <- -sum(dnorm(b, 0, sig.b, log=TRUE))
  lfb <- dnorm(b, 0, sig.b)
  lfb <- log(lfb)
  lfb[lfb==-Inf] <- -10
  lfb <- -sum(lfb)
  #compute and output negative joint log density
  lf <- lfy_b + lfb
  lf
}

#compute the derivative for   = 1
expr1 <- expression(k*(B0 + B1*rx + b.aux) + (y/exp(B0 + B1*rx + b.aux))^k)
nll.aux1 <- deriv(expr1, c("b.aux"), function.arg=c("b.aux", "B0", "B1", "k", "rx", "y"))

#status = 0
expr0 <- expression((y/exp(B0 + B1*rx + b.aux))^k)
nll.aux0 <- deriv(expr0, c("b.aux"), function.arg=c("b.aux", "B0", "B1", "k", "rx", "y"))

#numerically compute the gradient of lfyb
grad <- function (b, y, theta, X, Z) {
  #define variables
  rx <- X[,2]
  s <- X[,3]
  B0 <- theta[1]
  B1 <- theta[2]
  k <- 1/exp(theta[3])
  b.aux <- Z%*%b
  #gradient if status = 1
  b.grad1 <- nll.aux1(b.aux, B0, B1, k, rx, y)
  g1 <- attr(b.grad1, "gradient")
  g1 <- as.numeric(g1)
  #gradient if status = 0
  b.grad0 <- nll.aux0(b.aux, B0, B1, k, rx, y)
  g0 <- attr(b.grad0, "gradient")
  g0 <- as.numeric(g0)
  #select contribution to gradient based on status
  g <- s*g1 +(1-s)*g0
  #gradient of negative log likelihood of b
  b.grad <- b/exp(2*theta[4])
  m <- matrix(g, nrow=3, ncol=50)
  g <- colSums(m) + b.grad #add contributions from b terms
  g
}

lal <- function (theta, y, X, Z) {
  #compute the negative log likelihood of theta using the laplace approximation
  theta[theta<(-3)] <- -3
  theta[theta > 7] <- 7
  b <- rep(0,50)
  lfyb.opt <- optim(b, fn=lfyb, gr=grad, theta=theta, y=y, X=X, Z=Z, method="BFGS", hessian=TRUE)
  b.opt <- lfyb.opt$par
  hess <- lfyb.opt$hessian
  lapprox <- -(length(b.opt)/2)*log(2*pi) + (1/2)*log(det(hess)) + lfyb(b=b.opt, y=y, theta=theta, X=X, Z=Z)
  lapprox
}

lal.opt <- optim(theta, lal, y=y, X=X, Z=Z, method="BFGS", hessian=TRUE)
thetahat <- lal.opt$par
se <- sqrt(diag(solve(lal.opt$hessian)))
CI1 <- c(thetahat[1] + qnorm(0.025)*se[1], thetahat[1] + qnorm(0.975)*se[1])
CI.rand <- c(thetahat[2] + qnorm(0.025)*se[2], thetahat[2] + qnorm(0.975)*se[2])
CI3 <- c(thetahat[3] + qnorm(0.025)*se[3], thetahat[3] + qnorm(0.975)*se[3])
CI4 <- c(thetahat[4] + qnorm(0.025)*se[4], thetahat[4] + qnorm(0.975)*se[4])
CI <- matrix(c(CI1, CI.rand, CI3, CI4), nrow=2, ncol=4)

```


```{R}
lal <- function (theta, y, X, Z) {
  #compute the negative log likelihood of theta using the laplace approximation
  theta[theta<(-3)] <- -3
  theta[theta > 7] <- 7
  b <- rep(0,50)
  lfyb.opt <- optim(b, fn=lfyb, gr=grad, theta=theta, y=y, X=X, Z=Z, method="BFGS", hessian=TRUE)
  b.opt <- lfyb.opt$par
  hess <- lfyb.opt$hessian
  lapprox <- -(length(b.opt)/2)*log(2*pi) + (1/2)*log(det(hess)) + lfyb(b=b.opt, y=y, theta=theta, X=X, Z=Z)
  lapprox
}
 lal.opt <- optim(theta, lal, y=y, X=X, Z=Z, method="BFGS", hessian=TRUE)

```
The returned objects are displayed below.

```{r,echo=FALSE,message=FALSE}
  lal.opt <- optim(theta, lal, y=y, X=X, Z=Z, method="BFGS", hessian=TRUE)
  lal.opt

```

The optimal vector of parameters


The Weibull distribution adopted previously to model the effects of the added random effects, will be analysed again but this time following a Bayesian estimation procedure. The proposal distributions $q( \boldsymbol{\theta}_j | \boldsymbol{\theta}_{j-1})$ which were used to initiate the Metropolis Hastings method was chosen to be the marginal negative log-likelihood. The starting parameter $\boldsymbol{\theta}_0$ was taken to be the previously computed optimal values $\hat{\boldsymbol{\theta}}$ in the early stages of the analysis along with a small parameter estimate for $\log({\sigma}_b)$.

$$\boldsymbol{\theta}_0 = (4.9831505, -0.2384417, -1.3324342, 0)$$
The $R$ code used to define the initial value paramters can be seen below.

```{r}
#we set the seed for random generators for reproducibility
set.seed(4)

X <- model.matrix(~ rx + status, rats) #define X matrix
rats$litter <- factor(rats$litter)
Z <- model.matrix(~ litter - 1, rats) #define Z matrix
b <- rnorm(50,0,0.1) #set arbitrary random b
y <- rats$time #define y vector
theta <- c(4.9831505, -0.2384417, -1.3324342, 0) #using optimal paramater estimates from Q1, & small log(sig.b)
```

